{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrap Content and Build a Hugo Site"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps\n",
    "* Setup and Helper Functions\n",
    "* Import the Site Data from a CSV\n",
    "* For Each Site for teaser content, scrape the data (Title, Price and Url to the more information page)\n",
    "* Combine all the site data into a data frame\n",
    "* Export this data to a json and csv file for use on the hugo site and other projects (Drupal Import )\n",
    "* Build the hugo site"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the neccesary python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "import hashlib\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_full = datetime.today().strftime('%Y-%m-%d-%H:%M:')\n",
    "timestamp_day  = datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "# Site Import Data\n",
    "site_data_file = 'importfiles/sitedata.csv'\n",
    "site_data_df = pd.DataFrame(columns=[\"company\",\"site\", \"collection\"])\n",
    "\n",
    "# Site Export Data\n",
    "scraped_data_file = 'exportfiles/scraped_sites'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log processing commands to see where a site may have failed\n",
    "def log_processing(url):\n",
    "    print('  ---> processing ' + url)\n",
    "\n",
    "# Pull out everything but numbers and letters from the imported string\n",
    "def returnNumbersAndLettersOnly(oldString):\n",
    "    newString = ''.join(e for e in oldString if e.isalnum())\n",
    "    return newString\n",
    "\n",
    "# Pull out everything but numbers from the imported string\n",
    "def returnNumbersOnly(oldString):\n",
    "    newString = ''.join(e for e in oldString if e.isdigit())\n",
    "    return newString\n",
    "\n",
    "# Set default headers for call\n",
    "# Testing out calls \n",
    "def getHeadersObject(url):    \n",
    "    headers = {\n",
    "        'User-Agent': \"PostmanRuntime/7.18.0\",\n",
    "        'Accept': \"*/*\",\n",
    "        'Cache-Control': \"no-cache\",\n",
    "        'Accept-Encoding': \"gzip, deflate\",\n",
    "        'Referer': url,\n",
    "        'Connection': \"keep-alive\",\n",
    "        'cache-control': \"no-cache\"\n",
    "    }    \n",
    "    return headers\n",
    "    \n",
    "# Get the index of an item and handle the exception where the index does not exist and set to empty\n",
    "def pop(item,index):\n",
    "\n",
    "    try:\n",
    "        return item[index]\n",
    "    except IndexError:\n",
    "        return 'null'  \n",
    "        \n",
    "    return breakout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapeTeaserDataFromCollection_type1(url,site):\n",
    "\n",
    "    site_scraped_data_df_temp = pd.DataFrame(columns=[\"site\",\"title\", \"url\",\"price\"])    \n",
    "    soup = BeautifulSoup(requests.get(url).text, 'html.parser')\n",
    "    resultsRow = soup.find_all('article', {'class': 'box-info'})  \n",
    "   \n",
    "    for resultRow in resultsRow:\n",
    "        resultRowBreakdown = resultRow.find('p', {'class': 'text-info'}).text.split('\\n')\n",
    "        try:\n",
    "            site_scraped_data_df_temp = site_scraped_data_df_temp.append(\n",
    "                {\n",
    "                    'site':site,\n",
    "                    'title':pop(resultRowBreakdown,0),\n",
    "                    'price':returnNumbersOnly(pop(resultRowBreakdown,2)),\n",
    "                    'url':resultRow.find('a').get('href')\n",
    "                }\n",
    "            , ignore_index=True)\n",
    "        except IndexError as e:\n",
    "            gotdata = '' \n",
    "        \n",
    "    return site_scraped_data_df_temp\n",
    "       \n",
    "def scrapeTeaserDataFromCollection_type2(url,site):\n",
    "\n",
    "    site_scraped_data_df_temp = pd.DataFrame(columns=[\"site\",\"title\", \"url\",\"price\"])    \n",
    "    soup = BeautifulSoup(requests.get(url).text, 'html.parser')\n",
    "    resultsRow = soup.find('div', {'class': 'ppb_tour_classic'}).find_all('div', {'class': 'element'})  \n",
    "   \n",
    "    for resultRow in resultsRow:\n",
    "        try:\n",
    "            site_scraped_data_df_temp = site_scraped_data_df_temp.append(\n",
    "                {\n",
    "                    'site':site,\n",
    "                    'title':resultRow.find('h4').text, #<h2><a -.find('a'),\n",
    "                    'price':returnNumbersOnly(resultRow.find('div', {'class': 'tour_price'}).text.strip('$')),\n",
    "                    'url':resultRow.find('a',{'class': 'tour_link'}).get('href')   \n",
    "                }\n",
    "            , ignore_index=True)\n",
    "        except IndexError as e:\n",
    "            gotdata = '' \n",
    "        \n",
    "    return site_scraped_data_df_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the site data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>site</th>\n",
       "      <th>collection</th>\n",
       "      <th>scrape_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Example Site 1</td>\n",
       "      <td>https://www.examplesite.com</td>\n",
       "      <td>https://www.example.com/bloglist/</td>\n",
       "      <td>type1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Example Site 2</td>\n",
       "      <td>https://www.examplesite.com</td>\n",
       "      <td>https://www.example.com/bloglist/</td>\n",
       "      <td>type2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          company                         site  \\\n",
       "0  Example Site 1  https://www.examplesite.com   \n",
       "1  Example Site 2  https://www.examplesite.com   \n",
       "\n",
       "                          collection scrape_type  \n",
       "0  https://www.example.com/bloglist/       type1  \n",
       "1  https://www.example.com/bloglist/       type2  "
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import and Print out the Sites you want to Scrape\n",
    "site_data_df = pd.read_csv(site_data_file)\n",
    "site_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ---> processing Example Site 1\n",
      "NOT SCRAPE TYPE FOUND\n",
      "  ---> processing Example Site 2\n",
      "NOT SCRAPE TYPE FOUND\n"
     ]
    }
   ],
   "source": [
    "site_scraped_data_df = pd.DataFrame(columns=[\"site\",\"title\", \"url\",\"price\"])\n",
    "\n",
    "#Go Through All Sites and Scrape the Appropriate Data\n",
    "\n",
    "for index, row in site_data_df.iterrows():\n",
    "    log_processing(row['company'])\n",
    "    if row['scrape_type'] == 'type1':\n",
    "        site_scraped_data_df = site_scraped_data_df.append(scrapeTeaserDataFromCollection(row['collection'],row['company']), ignore_index=True)\n",
    "    elif row['scrape_type'] == 'type2':\n",
    "        site_scraped_data_df = site_scraped_data_df.append(scrapeTeaserDataFromCollection_type2(row['collection'],row['company']), ignore_index=True)\n",
    "    else:\n",
    "        print(\"NOT SCRAPE TYPE FOUND\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output results to CSV & JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [site, title, url, price]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#Output to CSV\n",
    "site_scraped_data_df.to_csv(scraped_data_file+'.csv') \n",
    "#Output to JSON (Orient will allow you to input this into Datatables.net structure for display on a site)\n",
    "site_scraped_data_df.to_json(scraped_data_file+'.json',orient='records')    \n",
    "print(site_scraped_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
